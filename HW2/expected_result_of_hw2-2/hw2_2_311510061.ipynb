{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.How do you choose the tokenizer for this task? Could we use the white space to tokenize\n",
    "the text? What about using the complicated tokenizer instead? Make some discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Eample Sentence:\n",
      "\"We make these sick people known worldwide for their horrifying acts, let’s stop that.\"\n",
      "\n",
      "NLTK EXAMPLE:\n",
      "['\"', 'We', 'make', 'these', 'sick', 'people', 'known', 'worldwide', 'for', 'their', 'horrifying', 'acts', ',', 'let', '’', 's', 'stop', 'that', '.\"']\n",
      "\n",
      "spaCy EXAMPLE:\n",
      "['\"', 'We', 'make', 'these', 'sick', 'people', 'known', 'worldwide', 'for', 'their', 'horrifying', 'acts', ',', 'let', '’s', 'stop', 'that', '.', '\"']\n",
      "\n",
      "Gensim EXAMPLE:\n",
      "['We', 'make', 'these', 'sick', 'people', 'known', 'worldwide', 'for', 'their', 'horrifying', 'acts', 'let', 's', 'stop', 'that']\n",
      "\n",
      "TorchText EXAMPLE:\n",
      "['jets', 'chairman', 'christopher', 'johnson', 'won', \"'\", 't', 'fine', 'players', 'for', 'anthem', 'protests']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import csv\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from gensim.utils import tokenize\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# OPEN CSV FILE\n",
    "with open(\"train.csv\") as csvfile:\n",
    "    \"\"\" LOAD CSV FILE \"\"\"\n",
    "    row_ex = list(csv.reader(csvfile))\n",
    "\n",
    "print(\"Original Eample Sentence:\")\n",
    "print(row_ex[15][3])\n",
    "print()\n",
    "\n",
    "print(\"NLTK EXAMPLE:\")\n",
    "print(wordpunct_tokenize(row_ex[15][3]))\n",
    "print()\n",
    "\n",
    "print(\"spaCy EXAMPLE:\")\n",
    "token_list = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_doc = nlp(row_ex[15][3])\n",
    "for token in spacy_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)\n",
    "print()\n",
    "\n",
    "print(\"Gensim EXAMPLE:\")\n",
    "print(list(tokenize(row_ex[15][3])))\n",
    "print()\n",
    "\n",
    "print(\"TorchText EXAMPLE:\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "print(tokenizer(row_ex[1][2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面列舉出了各種tokenizer的切割方式，個人的話我比較喜歡spacy的切法，但實際去用到transformer後似乎torchtext的效果會最好，\n",
    "可能比較適合機器的訓練與判讀，因此最後是使用torchtext作為tokenizer。使用空白做為切割也是一種方法，但不一定適用於所有的dataset，還是要看dataset的樣式來選擇。至於使用較複雜的切法則可能因為切得太細導致本來該在一起的單字被切割成了兩個而下降了訓練的品質。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Why we need the special tokens like ⟨pad⟩, ⟨unk⟩?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to insert additional \"special tokens\" such as ⟨PAD⟩, ⟨UNK⟩, ⟨STA⟩, ⟨END⟩ in some cases, which is not in the input text but have important meaning that we want the model to act on.\n",
    "\n",
    "⟨PAD⟩ Which is Padding token : Added to the end of shorter inputs so that all inputs have the same length. This is because inputs to a neural network model are typically batched and the model operates on entire batches.\n",
    "\n",
    "⟨UNK⟩ Which is Unknown token : Used to limit the number of distinct tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Briefly explain how your procedure is run to handle the text data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一開始先將資料寫入並進行tokenizer的切割，接著將資料包進dataloader並送進我們所架的transformer中進行model training，當train到一定的準確率後在將test資料送進model中預測出最相似的category並讀出csv檔。\n",
    "Setting 裡面較為重要的部分是我所選擇的tokenizer種類，作業中最後使用了torchtext，以及使用了glove做為embedding pretrain也提升了許多的準確率。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n",
      "'unzip' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torchtext.vocab as vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# OPEN CSV FILE\n",
    "with open(\"train.csv\") as csvfile:\n",
    "    \"\"\" LOAD CSV FILE \"\"\"\n",
    "    row_train = list(csv.reader(csvfile))\n",
    "\n",
    "with open(\"test.csv\") as csvfile:\n",
    "    \"\"\" LOAD CSV FILE \"\"\"\n",
    "    row_test = list(csv.reader(csvfile))\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "glove = vocab.GloVe(name = \"6B\", dim = 300, cache = '.vector_cache/glove')\n",
    "embedding_index = {}\n",
    "count = 0\n",
    "glove_data = open(\"glove.6B.300d.txt\", encoding = \"utf8\")\n",
    "for line in glove_data:\n",
    "    embedding_index[line.split()[0]] = count\n",
    "    count += 1\n",
    "glove_data.close()\n",
    "\n",
    "# def text_pipeline(x):\n",
    "    # return [embedding_index[str(token.text.lower())] if (str(token.text.lower()) in embedding_index) else 2 for token in nlp(x)]\n",
    "\n",
    "def text_pipeline(x):\n",
    "    return [embedding_index[str(token.lower())] if (str(token.lower()) in embedding_index) else glove.stoi[\"unk\"] for token in tokenizer(x)]\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(row_train) - 1):\n",
    "    train_data.append((row_train[i + 1][1], text_pipeline(row_train[i + 1][2] + \".\" + row_train[i + 1][3])))\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(row_test) - 1):\n",
    "    test_data.append((row_test[i + 1][1] + \".\" + row_test[i + 1][2]))\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for _, text in data:\n",
    "        token_list = []\n",
    "        spacy_doc = nlp(text)\n",
    "        for token in spacy_doc:\n",
    "            token_list.append(token.text)\n",
    "        yield token_list\n",
    "\n",
    "\"\"\"Run on GPU\"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_len = 60\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(_text, dtype = torch.int64)\n",
    "        pad = nn.ConstantPad1d((0, max_len - len(processed_text)), glove.stoi[\".\"])   \n",
    "        processed_text = pad(processed_text)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype = torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim = 0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(200, 200).to(device)\n",
    "        self.q = nn.Linear(200, 200).to(device)\n",
    "        self.v = nn.Linear(200, 200).to(device)\n",
    "        self.projection = nn.Linear(200, 200).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = rearrange(self.q(x), \"b n (h d) -> b h n d\", h=4)\n",
    "        k = rearrange(self.k(x), \"b n (h d) -> b h n d\", h=4)\n",
    "        v  = rearrange(self.v(x), \"b n (h d) -> b h n d\", h=4)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', q, k)\n",
    "        scaling = 200 ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "def generate_mask(len):\n",
    "    return torch.triu(torch.ones(len, len) * float(\"-inf\"), diagonal = 1).to(device)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, class_num, dropout = 0.2, nhead = 2, d_hid = 200, nlayers = 2):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, d_model, sparse = False) \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.swish = Swish()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.positional_encoder = PositionalEncoding(d_model) \n",
    "        self.num_encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout) \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.num_encoder_layers, nlayers) \n",
    "        self.decoder = nn.Linear(d_model, class_num) \n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.copy_(glove.vectors)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.normal_(mean = 0, std = 0.5)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # embedded = self.embedding(text, offsets)\n",
    "        # embedded = self.tanh(embedded)\n",
    "        # embedded = self.dropout(embedded)\n",
    "        out = self.embedding(text, offsets) * math.sqrt(self.d_model)\n",
    "        mask = generate_mask(out.shape[0])\n",
    "        out = self.positional_encoder(out)  \n",
    "        # out = self.swish(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.transformer_encoder(out, mask)\n",
    "        # out = self.transformer_encoder(out) \n",
    "        # out = self.swish(out)\n",
    "        # out = self.dropout(out)\n",
    "        # return self.decoder(embedded) \n",
    "        return self.decoder(out)  \n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate = 0.3):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads = num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            [nn.Dense(ff_dim, activation = \"tanh\"), nn.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = nn.LayerNormalization(epsilon = 1e-6)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.2, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # (5000, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.pe[:x.size(0)]  # (64, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log = 1\n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "\n",
    "    for index, (label, text, offsets) in enumerate(dataloader):\n",
    "        count = count + 1\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if index % log == 0 and index > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"| epoch: {:3d} | batch:{:4d}/{:4d}\"\n",
    "                \"| accuracy: {:5.3f}\".format(epoch + 1, index, len(dataloader), total_acc / total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            total_loss += criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count, total_loss\n",
    "\n",
    "\"\"\"MODEL SETTING\"\"\"\n",
    "class_num = 4\n",
    "nhead = 5  # number of heads in nn.MultiheadAttention\n",
    "d_model = 300 # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.3\n",
    "model = Transformer(len(glove), d_model, class_num, dropout = dropout, nhead = nhead, d_hid = d_hid, nlayers = nlayers).to(device)\n",
    "epochs = 600\n",
    "lr = 10\n",
    "batch_size = 40\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.99)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "total_accu = None\n",
    "num_train = int(len(train_data) * 0.999)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_data, [num_train, len(train_data) - num_train])\n",
    "\"\"\"MODEL SETTING\"\"\"\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size = batch_size,\n",
    "                            shuffle = True, collate_fn = collate_batch)\n",
    "# train_dataloader = DataLoader(train_data, batch_size = batch_size,\n",
    "#                             shuffle = True, collate_fn = collate_batch)\n",
    "val_dataloader = DataLoader(split_valid_, batch_size = batch_size,\n",
    "                            shuffle = True, collate_fn = collate_batch)\n",
    "\n",
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text)).to(device)\n",
    "        out = model(text, torch.tensor([0]).to(device))\n",
    "        return out.argmax(1).item() + 1\n",
    "\n",
    "predict_class = []\n",
    "id_list = list(range(1, 401))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val, val_loss = evaluate(val_dataloader)\n",
    "\n",
    "    if epoch + 1 == epochs:\n",
    "        for i in range(len(test_data)):\n",
    "            predict_class.append(predict(test_data[i])) \n",
    "        col1 = \"id\"\n",
    "        col2 = \"category\"\n",
    "        data = pd.DataFrame({col1:id_list, col2:predict_class})\n",
    "        data.to_csv(\"submission.csv\", index = False)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        lr_scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    # lr_scheduler.step()\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    print(\"| end of epoch: {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy: {:8.3f}\".format(epoch + 1, time.time() - epoch_start_time, accu_val))\n",
    "    print(\"val loss: %f\" %(val_loss))\n",
    "    print(\"LR: %f\" %lr_scheduler.get_last_lr()[0])\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the model structure or hyperparameter setting in your design. (5%) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model structure: Input -> Input embedding -> positional embedding -> nlayers times encoder -> decoder -> Linear -> categry。\n",
    "Parameter choose: d model = 300, nhead = 5, nlayers = 2, d_hid = 300, dropout = 0.3,batch_size = 40, epoch =600, learning rate = 10，經過測試dropout稍微提高可以減少部分overfitting的狀況，但若調太高則會train的不好，d_model的維度則是配合glove因此都選擇300維度，nlayers若是加上太多層會導致模型深度太深，會有train不起來的問題，因此適當的選擇2來用，最後我的transformer用128或256的batch size小果會比40和64的差，故推測小的batch size會更好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
