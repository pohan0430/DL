{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import SoftTemplate\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import pandas\n",
    "\n",
    "# Load data\n",
    "train_tsvreader = pandas.read_csv(\"train.tsv\", sep = '\\t')\n",
    "# print(train_tsvreader[\"sentence\"][0])\n",
    "# print(train_tsvreader[\"label\"][0])\n",
    "train_data = []\n",
    "# [TODO] \n",
    "# Load training data\n",
    "train_label = []\n",
    "for i in range(len(train_tsvreader)):\n",
    "    train_tmp = InputExample(text_a = train_tsvreader[\"sentence\"][i], label = int(train_tsvreader[\"label\"][i]))\n",
    "    train_data.append(train_tmp)\n",
    "    train_label.append(int(train_tsvreader[\"label\"][i]))\n",
    "# print(train_data)\n",
    "\n",
    "valid_tsvreader = pandas.read_csv(\"test.tsv\", sep = '\\t')\n",
    "test_data = []\n",
    "# [TODO] \n",
    "# Load testing data\n",
    "test_label = []\n",
    "for i in range(len(valid_tsvreader)):\n",
    "    test_tmp = InputExample(text_a = valid_tsvreader[\"sentence\"][i], label = int(valid_tsvreader[\"label\"][i]))\n",
    "    test_data.append(test_tmp)\n",
    "    test_label.append(int(valid_tsvreader[\"label\"][i]))\n",
    "# print(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm('bert', 'bert-base-uncased')\n",
    "\n",
    "template    = '{\"placeholder\":\"text_a\"} is so {\"mask\"}.'  # [TODO] Define a template\n",
    "soft_tokens = 20   # [TODO] number of soft tokens\n",
    "mytemplate  = SoftTemplate(model = plm, text = template, tokenizer = tokenizer,\n",
    "                    num_tokens = soft_tokens, initialize_from_vocab = True)\n",
    "\n",
    "classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
    "    \"0\",\n",
    "    \"1\"\n",
    "]\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"0\": [\"bad\",\"terrible\",\"disgusting\",\"horrible\"],\n",
    "        \"1\": [\"good\", \"wonderful\", \"great\",\"excellent\"],\n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "prompt_model = PromptForClassification(plm = plm, template = mytemplate, verbalizer = promptVerbalizer, freeze_plm = True)    #[TODO] Combine Template and Verbalizer into a PromptModel\n",
    "prompt_model=  prompt_model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 400it [00:00, 1523.39it/s]\n",
      "tokenizing: 100it [00:00, 1257.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset = train_data, template = mytemplate, tokenizer = tokenizer,\n",
    "    tokenizer_wrapper_class = WrapperClass, max_seq_length = 256, decoder_max_length = 3,\n",
    "    batch_size = 4, shuffle = True, teacher_forcing = False, predict_eos_token = False,\n",
    "    truncate_method = \"head\"\n",
    "    #[TODO] \n",
    ")\n",
    "valid_dataloader = PromptDataLoader(dataset = test_data, template = mytemplate, tokenizer = tokenizer,\n",
    "    tokenizer_wrapper_class = WrapperClass, max_seq_length = 256, decoder_max_length = 3,\n",
    "    batch_size = 4, shuffle = True, teacher_forcing = False, predict_eos_token = False,\n",
    "    truncate_method = \"head\"\n",
    "    #[TODO] \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abc89\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average loss: 0.4484763199463487\n",
      "Epoch 2, average loss: 0.38302979096770284\n",
      "Epoch 3, average loss: 0.2998768799006939\n",
      "Epoch 4, average loss: 0.27077757283113896\n",
      "Epoch 5, average loss: 0.23960866548120976\n",
      "Epoch 6, average loss: 0.1903487076656893\n",
      "Epoch 7, average loss: 0.1496634561708197\n",
      "Epoch 8, average loss: 0.13049698527436704\n",
      "Epoch 9, average loss: 0.09951501901261509\n",
      "Epoch 10, average loss: 0.08302601332776248\n",
      "Epoch 11, average loss: 0.08575434601632878\n",
      "Epoch 12, average loss: 0.056581757572712374\n",
      "Epoch 13, average loss: 0.0389613206370268\n",
      "Epoch 14, average loss: 0.014625103387224954\n",
      "Epoch 15, average loss: 0.00764112455479335\n",
      "FINAL RESULT:\n",
      "=========================\n",
      "Training Data:\n",
      "Final Loss: 0.46334\n",
      "Final Accuracy: 0.61250\n",
      "=========================\n",
      "Testing Data:\n",
      "Final Loss: 14.73317\n",
      "Final Accuracy: 0.63750\n",
      "=========================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AdamW\n",
    ")\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {\"params\": [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]# [TODO] freeze parameter of encoder, only update soft tokens \n",
    "\n",
    "# Using different optimizer for prompt parameters and model parameters\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n,p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr = 3e-3) # [TODO])\n",
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr = 3e-4)\n",
    "\n",
    "for epoch in range(15): # [TODO] number of epochs):\n",
    "    # [TODO]\n",
    "    tot_loss = 0.0\n",
    "    count = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        predict = prompt_model(inputs)\n",
    "        loss = criterion(predict, inputs[\"label\"])\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        count += 1\n",
    "    print(\"Epoch {}, average loss: {}\".format(epoch + 1, tot_loss / count))\n",
    "train_len = len(train_label)*0.8\n",
    "test_len = len(test_label)*0.8\n",
    "def evaluate(dataloader):\n",
    "    prompt_model.eval()\n",
    "    total_loss = 0\n",
    "    pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, inputs in enumerate(dataloader):\n",
    "            inputs = inputs.cuda()\n",
    "            pred = prompt_model(inputs)\n",
    "            pred_list.extend(torch.argmax(pred, dim = -1).cpu().tolist())\n",
    "            total_loss += criterion(pred, inputs[\"label\"])\n",
    "    return total_loss, pred_list\n",
    "\n",
    "def print_result(train_loss, train_acc, test_loss, test_acc):\n",
    "    \"\"\"RESULT\"\"\"\n",
    "    print(\"FINAL RESULT:\")\n",
    "    print(\"=\" * 25)\n",
    "    print(\"Training Data:\")\n",
    "    print(\"Final Loss: %.5f\" %train_loss) \n",
    "    print(\"Final Accuracy: %.5f\" %train_acc) \n",
    "    print(\"=\" * 25)\n",
    "    print(\"Testing Data:\")\n",
    "    print(\"Final Loss: %.5f\" %test_loss)    \n",
    "    print(\"Final Accuracy: %.5f\" %test_acc)      \n",
    "    print(\"=\" * 25)      \n",
    "    \"\"\"RESULT\"\"\"\n",
    "\n",
    "train_loss, train_pred = evaluate(train_dataloader)\n",
    "test_loss, test_pred = evaluate(valid_dataloader)\n",
    "\n",
    "train_acc = sum([int(i == j) for i, j in zip(train_pred, train_label)])/train_len\n",
    "test_acc = sum([int(i == j) for i, j in zip(test_pred, test_label)])/test_len\n",
    "print_result(train_loss, train_acc, test_loss, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
